{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLib - Machine learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "#from pyspark.ml.feature import Tokenizer\n",
    "#from pyspark.ml.feature import StopWordsRemover\n",
    "#from pyspark.ml.feature import HashingTF\n",
    "#from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output folders\n",
    "trainingData = \"trainingData.csv\"\n",
    "unlabeledData = \"unlabeledData.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Training step\n",
    "# *************************\n",
    "\n",
    "# Create a DataFrame from trainingData.csv\n",
    "# Training data in raw format\n",
    "trainingData = spark.read.load(trainingData,\\\n",
    "                     format=\"csv\",\\\n",
    "                     header=True,\\\n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData.printSchema()\n",
    "trainingData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- label: integer (nullable = true)\n",
    " |-- text: string (nullable = true)\n",
    "\n",
    "+-----+--------------------+\n",
    "|label|                text|\n",
    "+-----+--------------------+\n",
    "|    1|The Spark system ...|\n",
    "|    1|Spark is a new di...|\n",
    "|    0|Turin is a beauti...|\n",
    "|    0|Turin is in the n...|\n",
    "+-----+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function that returns the number of words occurring in the input string\n",
    "def countWords(text):\n",
    "    return len(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a UDF function associated with countWords\n",
    "# We explicitly report also the data type of the returned value\n",
    "spark.udf.register(\"countWords\", countWords, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Python function that checks if the input string contain the work \"Spark\"\n",
    "def containsSpark(text):\n",
    "    return text.find(\"Spark\")>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a UDF function associated with containsSpark\n",
    "# We explicitly report also the data type of the returned value\n",
    "spark.udf.register(\"containsSpark\", containsSpark, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the attributes label and text and create  two new columns:\n",
    "# numLines and SparkWord\n",
    "newFeaturesDF = trainingData\\\n",
    ".selectExpr(\"label\", \"text\", \"countWords(text)\", \"containsSpark(text)\")\\\n",
    ".withColumnRenamed(\"countWords(text)\", \"numLines\")\\\n",
    ".withColumnRenamed(\"containsSpark(text)\", \"SparkWord\")\n",
    "\n",
    "# OR\n",
    "# Create an SQLTransformer to add two column to the input dataframe:\n",
    "# numLines and SparkWord\n",
    "sqlTrans = SQLTransformer(statement=\"\"\"SELECT *,\n",
    "countWords(text) AS numLines,\n",
    "containsSpark(text) AS SparkWord\n",
    "FROM __THIS__\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeaturesDF.printSchema()\n",
    "newFeaturesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- label: integer (nullable = true)\n",
    " |-- text: string (nullable = true)\n",
    " |-- numLines: integer (nullable = true)\n",
    " |-- SparkWord: boolean (nullable = true)\n",
    "\n",
    "+-----+--------------------+--------+---------+\n",
    "|label|                text|numLines|SparkWord|\n",
    "+-----+--------------------+--------+---------+\n",
    "|    1|The Spark system ...|       7|     true|\n",
    "|    1|Spark is a new di...|       6|     true|\n",
    "|    0|Turin is a beauti...|       5|    false|\n",
    "|    0|Turin is in the n...|       8|    false|\n",
    "+-----+--------------------+--------+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use an assembler to combine \"numLines\" and \"SparkWord\" in a Vector\n",
    "assembler = VectorAssembler(inputCols=[\"numLines\", \"SparkWord\"],\\\n",
    "                            outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification model based on the logistic regression algorithm\n",
    "# We can set the values of the parameters of the \n",
    "# Logistic Regression algorithm using the setter methods.\n",
    "lr = LogisticRegression()\\\n",
    ".setMaxIter(10)\\\n",
    ".setRegParam(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline that is used to create the logistic regression\n",
    "# model on the training data.\n",
    "pipeline = Pipeline().setStages([assembler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the pipeline on the training data to build the \n",
    "# classification model\n",
    "classificationModel = pipeline.fit(newFeaturesDF)\n",
    "\n",
    "# Now, the classification model can be used to predict the class label\n",
    "# of new unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************\n",
    "# Prediction  step\n",
    "# *************************\n",
    "# Read unlabeled data\n",
    "# Create a DataFrame from unlabeledData.csv\n",
    "# Unlabeled data in raw format\n",
    "unlabeledData = spark.read.load(unlabeledData,\\\n",
    "                     format=\"csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlabeledData.printSchema()\n",
    "#unlabeledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeaturesDFunlabeled = unlabeledData\\\n",
    ".selectExpr(\"label\", \"text\", \"countWords(text)\", \"containsSpark(text)\")\\\n",
    ".withColumnRenamed(\"countWords(text)\", \"numLines\")\\\n",
    ".withColumnRenamed(\"containsSpark(text)\", \"SparkWord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newFeaturesDFunlabeled.printSchema()\n",
    "newFeaturesDFunlabeled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- label: string (nullable = true)\n",
    " |-- text: string (nullable = true)\n",
    " |-- numLines: integer (nullable = true)\n",
    " |-- SparkWord: boolean (nullable = true)\n",
    "\n",
    "+-----+--------------------+--------+---------+\n",
    "|label|                text|numLines|SparkWord|\n",
    "+-----+--------------------+--------+---------+\n",
    "| null|Spark performs be...|       5|     true|\n",
    "| null|Comparison betwee...|       5|     true|\n",
    "| null|Turin is in Piedmont|       4|    false|\n",
    "+-----+--------------------+--------+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on unlabeled documents by using the \n",
    "# Transformer.transform() method.\n",
    "# The transform will only use the 'features' columns\n",
    "predictionsDF = classificationModel.transform(newFeaturesDFunlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsDF.printSchema()\n",
    "predictionsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- label: string (nullable = true)\n",
    " |-- text: string (nullable = true)\n",
    " |-- numLines: integer (nullable = true)\n",
    " |-- SparkWord: boolean (nullable = true)\n",
    " |-- features: vector (nullable = true)\n",
    " |-- rawPrediction: vector (nullable = true)\n",
    " |-- probability: vector (nullable = true)\n",
    " |-- prediction: double (nullable = false)\n",
    "\n",
    "+-----+--------------------+--------+---------+---------+--------------------+--------------------+----------+\n",
    "|label|                text|numLines|SparkWord| features|       rawPrediction|         probability|prediction|\n",
    "+-----+--------------------+--------+---------+---------+--------------------+--------------------+----------+\n",
    "| null|Spark performs be...|       5|     true|[5.0,1.0]|[-3.1272480248757...|[0.04199718899423...|       1.0|\n",
    "| null|Comparison betwee...|       5|     true|[5.0,1.0]|[-3.1272480248757...|[0.04199718899423...|       1.0|\n",
    "| null|Turin is in Piedmont|       4|    false|[4.0,0.0]|[3.19966999960023...|[0.96082185681571...|       0.0|\n",
    "+-----+--------------------+--------+---------+---------+--------------------+--------------------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The returned DataFrame has the following schema (attributes)\n",
    "# |-- label: string (nullable = true)\n",
    "# |-- text: string (nullable = true)\n",
    "# |-- words: array (nullable = true)\n",
    "# |    |-- element: string (containsNull = true)\n",
    "# |-- filteredWords: array (nullable = true)\n",
    "# |    |-- element: string (containsNull = true)\n",
    "# |-- rawFeatures: vector (nullable = true)\n",
    "# |-- features: vector (nullable = true)\n",
    "# |-- rawPrediction: vector (nullable = true)\n",
    "# |-- probability: vector (nullable = true)\n",
    "# |-- prediction: double (nullable = false)\n",
    "\n",
    "# Select only the original features (i.e., the value of the original text attribute) and \n",
    "# the predicted class for each record\n",
    "predictions = predictionsDF.select(\"text\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.printSchema()\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- text: string (nullable = true)\n",
    " |-- prediction: double (nullable = false)\n",
    "\n",
    "+-----------------------------------+----------+\n",
    "|text                               |prediction|\n",
    "+-----------------------------------+----------+\n",
    "|Spark performs better than Hadoop  |1.0       |\n",
    "|Comparison between Spark and Hadoop|1.0       |\n",
    "|Turin is in Piedmont               |0.0       |\n",
    "+-----------------------------------+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in an HDFS output folder\n",
    "predictions.write.csv(outputPath, header=\"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the number of followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathVertexes = \"vertexes.csv\"\n",
    "inputPathEdges = \"edges.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of vertexes.csv\n",
    "vDF = spark.read.load(inputPathVertexes,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vDF.printSchema()\n",
    "vDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- age: integer (nullable = true)\n",
    "\n",
    "+---+-----+---+\n",
    "| id| name|age|\n",
    "+---+-----+---+\n",
    "| u1|Alice| 34|\n",
    "| u2|  Bob| 36|\n",
    "| u3| John| 30|\n",
    "| u4|David| 29|\n",
    "| u5| Paul| 32|\n",
    "| u6| Adel| 36|\n",
    "| u7| Eddy| 60|\n",
    "+---+-----+---+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of edges.csv\n",
    "eDF = spark.read.load(inputPathEdges,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eDF.printSchema()\n",
    "eDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- src: string (nullable = true)\n",
    " |-- dst: string (nullable = true)\n",
    " |-- linktype: string (nullable = true)\n",
    "\n",
    "+---+---+--------+\n",
    "|src|dst|linktype|\n",
    "+---+---+--------+\n",
    "| u1| u2|  friend|\n",
    "| u1| u4|  friend|\n",
    "| u1| u5|  friend|\n",
    "| u2| u1|  friend|\n",
    "| u2| u3|  follow|\n",
    "| u3| u2|  follow|\n",
    "| u4| u1|  friend|\n",
    "| u4| u5|  friend|\n",
    "| u5| u1|  friend|\n",
    "| u5| u4|  friend|\n",
    "| u5| u6|  follow|\n",
    "| u6| u3|  follow|\n",
    "| u7| u6|  follow|\n",
    "+---+---+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the \"follow\" edges are useful\n",
    "# Filter the input edge dataframe before creating the graph\n",
    "filteredEdges = eDF.filter(\"linktype='follow' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input graph\n",
    "g = GraphFrame(vDF, filteredEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of followers for each user (i.e., the number of in-links) \n",
    "userNumFollowersDF = g.inDegrees.withColumnRenamed(\"inDegree\",\"numFollowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userNumFollowersDF.printSchema()\n",
    "userNumFollowersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- numFollowers: integer (nullable = false)\n",
    "\n",
    "+---+------------+\n",
    "| id|numFollowers|\n",
    "+---+------------+\n",
    "| u3|           2|\n",
    "| u6|           2|\n",
    "| u2|           1|\n",
    "+---+------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "userNumFollowersDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract users with the maximum number of followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathVertexes = \"vertexes.csv\"\n",
    "inputPathEdges = \"edges.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of vertexes.csv\n",
    "vDF = spark.read.load(inputPathVertexes,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of edges.csv\n",
    "eDF = spark.read.load(inputPathEdges,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the \"follow\" edges are useful\n",
    "# Filter the input edge dataframe before creating the graph\n",
    "filteredEdfes = eDF.filter(\"linktype='follow' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input graph\n",
    "g = GraphFrame(vDF, filteredEdfes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of followers for each user (i.e., the number of in-links) \n",
    "userNumFollowersDF = g.inDegrees.withColumnRenamed(\"inDegree\",\"numFollowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the user(s) with the maximum number of followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum value of \"number of followers\"\n",
    "maxFollowersDF = userNumFollowersDF.agg({\"numFollowers\":\"max\"})\\\n",
    ".withColumnRenamed(\"max(numFollowers)\",\"maxFollowers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the single record/Row of the DataFrame maxFollowersDF\n",
    "rowMaxNumFollowers = maxFollowersDF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the maximum number of followers from rowMaxNumFollowers\n",
    "maxNumFollowers=rowMaxNumFollowers.maxFollowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the user(s) with the maximum number of followers\n",
    "selectedUsersDF=userNumFollowersDF.filter(userNumFollowersDF.numFollowers==maxNumFollowers)\n",
    "# Or\n",
    "#electedUsersDF=userNumFollowersDF.filter(\"numFollowers=\"+str(maxNumFollowers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedUsersDF.printSchema()\n",
    "selectedUsersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- numFollowers: integer (nullable = false)\n",
    "\n",
    "+---+------------+\n",
    "| id|numFollowers|\n",
    "+---+------------+\n",
    "| u3|           2|\n",
    "| u6|           2|\n",
    "+---+------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "selectedUsersDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motif finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathVertexes = \"vertexes.csv\"\n",
    "inputPathEdges = \"edges.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of vertexes.csv\n",
    "vDF = spark.read.load(inputPathVertexes,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of edges.csv\n",
    "eDF = spark.read.load(inputPathEdges,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only links of type friend\n",
    "filteredEdges = eDF.filter(\"linktype='friend' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input graph\n",
    "g = GraphFrame(vDF, filteredEdges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the paths/motifs (v1)-[]->(v2);!(v2)-[]->(v1)\n",
    "selectedPaths = g.find(\"(userx)-[]->(usery);!(usery)-[]->(userx)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPaths.printSchema()\n",
    "selectedPaths.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- userx: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |    |-- age: integer (nullable = true)\n",
    " |-- usery: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |    |-- age: integer (nullable = true)\n",
    "\n",
    "+---------------+---------------+\n",
    "|          userx|          usery|\n",
    "+---------------+---------------+\n",
    "|[u4, David, 29]|[u1, Alice, 34]|\n",
    "|[u1, Alice, 34]|  [u2, Bob, 36]|\n",
    "+---------------+---------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the ids of the users (rename the selected columns)\n",
    "selectedPairsDF=selectedPaths.selectExpr(\"userx.id as IdFriend\",\"usery.id as IdNotFriend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPairsDF.printSchema()\n",
    "selectedPairsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- IdFriend: string (nullable = true)\n",
    " |-- IdNotFriend: string (nullable = true)\n",
    "\n",
    "+--------+-----------+\n",
    "|IdFriend|IdNotFriend|\n",
    "+--------+-----------+\n",
    "|      u4|         u1|\n",
    "|      u1|         u2|\n",
    "+--------+-----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "selectedPairsDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select users and topics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathVertexes = \"vertexes.csv\"\n",
    "inputPathEdges = \"edges.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of vertexes.csv\n",
    "vDF = spark.read.load(inputPathVertexes,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vDF.printSchema()\n",
    "vDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- entityName: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "\n",
    "+---+----------+--------+\n",
    "| id|entityName|    name|\n",
    "+---+----------+--------+\n",
    "| V1|      user|   Paolo|\n",
    "| V2|     topic|     SQL|\n",
    "| V3|      user|   David|\n",
    "| V4|     topic|Big Data|\n",
    "| V5|      user|    John|\n",
    "+---+----------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of edges.csv\n",
    "eDF = spark.read.load(inputPathEdges,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eDF.printSchema()\n",
    "eDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- src: string (nullable = true)\n",
    " |-- dst: string (nullable = true)\n",
    " |-- linktype: string (nullable = true)\n",
    "\n",
    "+---+---+----------+\n",
    "|src|dst|  linktype|\n",
    "+---+---+----------+\n",
    "| V1| V2|      like|\n",
    "| V1| V3|    follow|\n",
    "| V1| V4|    follow|\n",
    "| V3| V2|      like|\n",
    "| V3| V4|      like|\n",
    "| V5| V2|  expertOf|\n",
    "| V2| V4|correlated|\n",
    "| V4| V2|correlated|\n",
    "+---+---+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the \"follow\" edges are useful\n",
    "# Filter the input edge dataframe before creating the graph\n",
    "filteredEdfes = eDF.filter(\"linktype='follow' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input graph\n",
    "g = GraphFrame(vDF, filteredEdfes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsDF = g.find(\"(v1)-[]->(v2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsDF.printSchema()\n",
    "pathsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- v1: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |-- v2: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    "\n",
    "+-----------------+--------------------+\n",
    "|               v1|                  v2|\n",
    "+-----------------+--------------------+\n",
    "|[V1, user, Paolo]|   [V3, user, David]|\n",
    "|[V1, user, Paolo]|[V4, topic, Big D...|\n",
    "|[V3, user, David]|    [V2, topic, SQL]|\n",
    "|[V3, user, David]|[V4, topic, Big D...|\n",
    "+-----------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the pair (user,topic)\n",
    "selectedPathsDF = pathsDF.filter(\"v1.entityName='user' AND v2.entityName='topic' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPathsDF.printSchema()\n",
    "selectedPathsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- v1: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |-- v2: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    "\n",
    "+-----------------+--------------------+\n",
    "|               v1|                  v2|\n",
    "+-----------------+--------------------+\n",
    "|[V1, user, Paolo]|[V4, topic, Big D...|\n",
    "|[V3, user, David]|    [V2, topic, SQL]|\n",
    "|[V3, user, David]|[V4, topic, Big D...|\n",
    "+-----------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select name of the user and \"name\" of the topic\n",
    "userTopicDF = selectedPathsDF.selectExpr(\"v1.name as username\", \"v2.name as topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userTopicDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+--------+--------+\n",
    "|username|   topic|\n",
    "+--------+--------+\n",
    "|   Paolo|Big Data|\n",
    "|   David|     SQL|\n",
    "|   David|Big Data|\n",
    "+--------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "userTopicDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter users by edge type and topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathVertexes = \"vertexes.csv\"\n",
    "inputPathEdges = \"edges.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of vertexes.csv\n",
    "vDF = spark.read.load(inputPathVertexes,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vDF.printSchema()\n",
    "vDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- entityName: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "\n",
    "+---+----------+--------+\n",
    "| id|entityName|    name|\n",
    "+---+----------+--------+\n",
    "| V1|      user|   Paolo|\n",
    "| V2|     topic|     SQL|\n",
    "| V3|      user|   David|\n",
    "| V4|     topic|Big Data|\n",
    "| V5|      user|    John|\n",
    "+---+----------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of edges.csv\n",
    "eDF = spark.read.load(inputPathEdges,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eDF.printSchema()\n",
    "eDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- src: string (nullable = true)\n",
    " |-- dst: string (nullable = true)\n",
    " |-- linktype: string (nullable = true)\n",
    "\n",
    "+---+---+----------+\n",
    "|src|dst|  linktype|\n",
    "+---+---+----------+\n",
    "| V1| V2|      like|\n",
    "| V1| V3|    follow|\n",
    "| V1| V4|    follow|\n",
    "| V3| V2|    follow|\n",
    "| V3| V4|    follow|\n",
    "| V5| V2|  expertOf|\n",
    "| V2| V4|correlated|\n",
    "| V4| V2|correlated|\n",
    "+---+---+----------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the \"follow\"  and \"correlated\" edges are useful\n",
    "# Filter the input edge dataframe before creating the graph\n",
    "filteredEdfes = eDF.filter(\"linktype='follow' OR linktype='correlated' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input graph\n",
    "g = GraphFrame(vDF, filteredEdfes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsDF = g.find(\"(v1)-[e1]->(v2);(v2)-[e2]->(v3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathsDF.printSchema()\n",
    "pathsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- v1: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |-- e1: struct (nullable = false)\n",
    " |    |-- src: string (nullable = true)\n",
    " |    |-- dst: string (nullable = true)\n",
    " |    |-- linktype: string (nullable = true)\n",
    " |-- v2: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |-- e2: struct (nullable = false)\n",
    " |    |-- src: string (nullable = true)\n",
    " |    |-- dst: string (nullable = true)\n",
    " |    |-- linktype: string (nullable = true)\n",
    " |-- v3: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    "\n",
    "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
    "|                  v1|                  e1|                  v2|                  e2|                  v3|\n",
    "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
    "|   [V1, user, Paolo]|    [V1, V3, follow]|   [V3, user, David]|    [V3, V4, follow]|[V4, topic, Big D...|\n",
    "|   [V1, user, Paolo]|    [V1, V3, follow]|   [V3, user, David]|    [V3, V2, follow]|    [V2, topic, SQL]|\n",
    "|   [V1, user, Paolo]|    [V1, V4, follow]|[V4, topic, Big D...|[V4, V2, correlated]|    [V2, topic, SQL]|\n",
    "|   [V3, user, David]|    [V3, V2, follow]|    [V2, topic, SQL]|[V2, V4, correlated]|[V4, topic, Big D...|\n",
    "|   [V3, user, David]|    [V3, V4, follow]|[V4, topic, Big D...|[V4, V2, correlated]|    [V2, topic, SQL]|\n",
    "|    [V2, topic, SQL]|[V2, V4, correlated]|[V4, topic, Big D...|[V4, V2, correlated]|    [V2, topic, SQL]|\n",
    "|[V4, topic, Big D...|[V4, V2, correlated]|    [V2, topic, SQL]|[V2, V4, correlated]|[V4, topic, Big D...|\n",
    "+--------------------+--------------------+--------------------+--------------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the triples user -> follow -> topic -> correlated -> topic=\"Big data\"\n",
    "selectedPathsDF = pathsDF.filter(\"\"\"v1.entityName='user' \n",
    "AND e1.linktype='follow'\n",
    "AND v2.entityName='topic'\n",
    "AND e2.linktype='correlated'\n",
    "AND v3.entityName='topic' AND v3.name='Big Data' \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPathsDF.printSchema()\n",
    "selectedPathsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- v1: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |-- e1: struct (nullable = false)\n",
    " |    |-- src: string (nullable = true)\n",
    " |    |-- dst: string (nullable = true)\n",
    " |    |-- linktype: string (nullable = true)\n",
    " |-- v2: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    " |-- e2: struct (nullable = false)\n",
    " |    |-- src: string (nullable = true)\n",
    " |    |-- dst: string (nullable = true)\n",
    " |    |-- linktype: string (nullable = true)\n",
    " |-- v3: struct (nullable = false)\n",
    " |    |-- id: string (nullable = true)\n",
    " |    |-- entityName: string (nullable = true)\n",
    " |    |-- name: string (nullable = true)\n",
    "\n",
    "+-----------------+----------------+----------------+--------------------+--------------------+\n",
    "|               v1|              e1|              v2|                  e2|                  v3|\n",
    "+-----------------+----------------+----------------+--------------------+--------------------+\n",
    "|[V3, user, David]|[V3, V2, follow]|[V2, topic, SQL]|[V2, V4, correlated]|[V4, topic, Big D...|\n",
    "+-----------------+----------------+----------------+--------------------+--------------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select name of the selected users\n",
    "usersDF = selectedPathsDF.selectExpr(\"v1.name as username\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+--------+\n",
    "|username|\n",
    "+--------+\n",
    "|   David|\n",
    "+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "usersDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select closest nodes to user1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPathVertexes = \"vertexes.csv\"\n",
    "inputPathEdges = \"edges.csv\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of vertexes.csv\n",
    "vDF = spark.read.load(inputPathVertexes,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vDF.printSchema()\n",
    "vDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- age: integer (nullable = true)\n",
    "\n",
    "+---+-----+---+\n",
    "| id| name|age|\n",
    "+---+-----+---+\n",
    "| u1|Alice| 34|\n",
    "| u2|  Bob| 36|\n",
    "| u3| John| 30|\n",
    "| u4|David| 29|\n",
    "| u5| Paul| 32|\n",
    "| u6| Adel| 36|\n",
    "| u7| Eddy| 60|\n",
    "+---+-----+---+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of edges.csv\n",
    "eDF = spark.read.load(inputPathEdges,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eDF.printSchema()\n",
    "eDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- src: string (nullable = true)\n",
    " |-- dst: string (nullable = true)\n",
    " |-- linktype: string (nullable = true)\n",
    "\n",
    "+---+---+--------+\n",
    "|src|dst|linktype|\n",
    "+---+---+--------+\n",
    "| u1| u2|  friend|\n",
    "| u1| u4|  friend|\n",
    "| u1| u5|  friend|\n",
    "| u2| u1|  friend|\n",
    "| u2| u3|  follow|\n",
    "| u3| u2|  follow|\n",
    "| u4| u1|  friend|\n",
    "| u4| u5|  friend|\n",
    "| u5| u1|  friend|\n",
    "| u5| u4|  friend|\n",
    "| u5| u6|  follow|\n",
    "| u6| u3|  follow|\n",
    "+---+---+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input graph\n",
    "g = GraphFrame(vDF, eDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for each vertex the length of the shortest path to u1\n",
    "shortPathsLengDF = g.shortestPaths(['u1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortPathsLengDF.printSchema()\n",
    "shortPathsLengDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- age: integer (nullable = true)\n",
    " |-- distances: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: integer (valueContainsNull = false)\n",
    "\n",
    "+---+-----+---+---------+\n",
    "| id| name|age|distances|\n",
    "+---+-----+---+---------+\n",
    "| u6| Adel| 36|[u1 -> 3]|\n",
    "| u3| John| 30|[u1 -> 2]|\n",
    "| u2|  Bob| 36|[u1 -> 1]|\n",
    "| u4|David| 29|[u1 -> 1]|\n",
    "| u5| Paul| 32|[u1 -> 1]|\n",
    "| u1|Alice| 34|[u1 -> 0]|\n",
    "| u7| Eddy| 60|       []|\n",
    "+---+-----+---+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the users who can reach u1 in less than 3 \"hops\"\n",
    "selectedUsersDF=shortPathsLengDF.filter(\"distances['u1']<3 AND id<>'u1' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedUsersDF.printSchema()\n",
    "selectedUsersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- id: string (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    " |-- age: integer (nullable = true)\n",
    " |-- distances: map (nullable = true)\n",
    " |    |-- key: string\n",
    " |    |-- value: integer (valueContainsNull = false)\n",
    "\n",
    "+---+-----+---+---------+\n",
    "| id| name|age|distances|\n",
    "+---+-----+---+---------+\n",
    "| u3| John| 30|[u1 -> 2]|\n",
    "| u2|  Bob| 36|[u1 -> 1]|\n",
    "| u4|David| 29|[u1 -> 1]|\n",
    "| u5| Paul| 32|[u1 -> 1]|\n",
    "+---+-----+---+---------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with Columns name and numHops\n",
    "usersNameNumHopsDF=selectedUsersDF.selectExpr(\"name\", \"distances['u1'] AS numHops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersNameNumHopsDF.printSchema()\n",
    "usersNameNumHopsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- numHops: integer (nullable = true)\n",
    "\n",
    "+-----+-------+\n",
    "| name|numHops|\n",
    "+-----+-------+\n",
    "| John|      2|\n",
    "|  Bob|      1|\n",
    "|David|      1|\n",
    "| Paul|      1|\n",
    "+-----+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "usersNameNumHopsDF.write.csv(outputPath, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
