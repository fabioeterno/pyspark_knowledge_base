{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe\n",
    "compute the maximum value associated to each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath  = \"./\" \n",
    "outputPath = \"./\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "readingDF = spark.read.load(inputPath,\\\n",
    "                            format=\"csv\",\\\n",
    "                            header=False,\\\n",
    "                            inferSchema=True)\\\n",
    ".withColumnRenamed(\"_c0\", \"sensorId\")\\\n",
    ".withColumnRenamed(\"_c1\", \"date\")\\\n",
    ".withColumnRenamed(\"_c2\", \"PM10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readingDF.printSchema()\n",
    "#readingDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one group for each value of sensorId and compute the maximum value in each group\n",
    "sensorsMaxValueDF = readingDF.groupBy(\"sensorId\").agg({\"PM10\": \"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensorsMaxValueDF.printSchema()\n",
    "#sensorsMaxValueDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result in the output folder\n",
    "sensorsMaxValueDF.write.csv(outputPath, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "count the number of critical values (pm10>50) for each sensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\" \n",
    "outputPath = \"./\" \n",
    "readingsRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a filter transformation to select only the lines with PM10>50\n",
    "readingsHighValueRDD = readingsRDD.filter(lambda PM10Reading: float(PM10Reading.split(',')[2])>50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of key-value pairs\n",
    "# Each pair contains a sensorId (key) and +1 (value)\n",
    "# The function of the map transformation returns a tuple\n",
    "sensorsPM10CriticalValuesRDD = readingsHighValueRDD.map(lambda PM10Reading: (PM10Reading.split(',')[0], 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of critical values for each sensor by using the reduceByKey transformation.\n",
    "sensorsCountsRDD = sensorsPM10CriticalValuesRDD.reduceByKey(lambda value1, value2: value1+value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "count the number of critical values (pm10>50) for each sensor and keep only the sensors which have more than one critical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath  = \"./\" \n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame \n",
    "readingDF = spark.read.load(inputPath,\\\n",
    "                            format=\"csv\",\\\n",
    "                            header=False,\\\n",
    "                            inferSchema=True)\\\n",
    ".withColumnRenamed(\"_c0\", \"sensorId\")\\\n",
    ".withColumnRenamed(\"_c1\", \"date\")\\\n",
    ".withColumnRenamed(\"_c2\", \"PM10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readingDF.printSchema()\n",
    "#readingDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"register\" readingDF\n",
    "readingDF.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one group for each value of sensorId and count the number of records with PM10>50.\n",
    "# Select only the sensors with at least two high values\n",
    "sensorsMaxValueDF = spark.sql(\"\"\"SELECT sensorId, count(*)\n",
    "FROM readings\n",
    "WHERE PM10>50\n",
    "GROUP BY sensorId\n",
    "HAVING count(*)>=2\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sensorsMaxValueDF.printSchema()\n",
    "#sensorsMaxValueDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result in the output folder\n",
    "sensorsMaxValueDF.write.csv(outputPath, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain for each sensor the list of date with a critical value (pm10>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\" \n",
    "outputPath = \"./\"\n",
    "readingsRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a filter transformation to select only the lines with PM10>50\n",
    "readingsHighValueRDD = readingsRDD.filter(lambda PM10Reading: float(PM10Reading.split(',')[2])>50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of key-value pairs\n",
    "# Each pair contains a sensorId (key) and a date (value)\n",
    "sensorsCriticalDatesRDD = readingsHighValueRDD.map(lambda PM10Reading: (PM10Reading.split(',')[0], \n",
    "                                                                        PM10Reading.split(',')[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one pair for each sensor (key) with the list of dates associated with that sensor (value)\n",
    "# by using the groupByKey transformation\n",
    "finalSensorCriticalDates = sensorsCriticalDatesRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The map method is used to transform the content of the iterable \n",
    "# over the values of each key into a list (that can be stored in a readable format)\n",
    "finalSensorCriticalDateStringFormat = finalSensorCriticalDates.mapValues(lambda dates : list(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result in the output folder\n",
    "finalSensorCriticalDateStringFormat.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain for each sensor the list of date with a critical value (pm10>50), including the sensor without critical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\" \n",
    "outputPath = \"./\"\n",
    "inputRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the lines with a pm10 value greater than 50\n",
    "highPM10RDDValues = inputRDD.filter(lambda line : float(line.split(',')[-1]) > 50)\\\n",
    "                    .map(lambda line : (line.split(',')[0], line.split(',')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point we apply groupByKey() to concatenate the date in a list for the same key\n",
    "highPM10RDDValuesGroupByKey = highPM10RDDValues.groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to apply mapValues to transform the value part\n",
    "highPM10RDDmapValues = highPM10RDDValuesGroupByKey.mapValues(lambda dates : list(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am missing the sensor which have no pm10 > 50, so I need to subtract from the input RDD the keys of the \n",
    "# RDD where I have the sensor with higher pm10 values\n",
    "neverHighPM10RDD = inputRDD.map(lambda v1 : v1.split(',')[0]).subtract(highPM10RDDmapValues.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neverHighPM10RDD = neverHighPM10RDD.map(lambda v1 : (v1, []))\n",
    "neverHighPM10RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRDD = highPM10RDDmapValues.union(neverHighPM10RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRDD.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order sensors with the respect to the critical values in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\" \n",
    "outputPath = \"./\"\n",
    "inputRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows which pm10 value is greater than 50\n",
    "filteredRDD = inputRDD.filter(lambda line : float(line.split(',')[-1]) > 50)\n",
    "filteredRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emit key value pair (sid, +1)\n",
    "sensorIdCounterRDD = filteredRDD.map(lambda s : (s.split(',')[0], 1))\n",
    "sensorIdCounterRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I reduceByKey the previous RDD to aggregate the values\n",
    "sensorIdCounterRDDAggr = sensorIdCounterRDD.reduceByKey(lambda v1,v2 : v1+v2)\n",
    "sensorIdCounterRDDAggr.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to order the RDD, I can apply sortBy\n",
    "sortedRDD = sensorIdCounterRDDAggr.sortBy(lambda pair : pair[1], False)\n",
    "sortedRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedRDD.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the sensor with the maximum value of pm10 associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\" \n",
    "outputPath = \"./\"\n",
    "inputRDD = sc.textFile(inputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highPM10RDD = inputRDD.filter(lambda line : float(line.split(',')[-1]) > 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I emit the key-value pairs (sId, +1)\n",
    "keyValueSensorRDD = highPM10RDD.map(lambda line : (line.split(',')[0], 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I reduceByKey the RDD\n",
    "counterSensor = keyValueSensorRDD.reduceByKey(lambda v1,v2 : v1+v2).top(1, lambda pair : pair[1])\n",
    "# This is a python variable\n",
    "counterSensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an RDD\n",
    "counterSensorRDD = sc.parallelize(counterSensor, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterSensorRDD.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Associate to each question id the questions and the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "questions  = \"./\"\n",
    "answers  = \"./\"\n",
    "outputPath = \"./\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsRDD = sc.textFile(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questionsRDD.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['Q1,2015-01-01,What is ..?', 'Q2,2015-01-03,Who invented ..']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am interested in the questionid and the question, I apply a map function, \n",
    "# I split the file and I take the first and third piece\n",
    "questionRDDFiltered = questionsRDD.map(lambda line : (line.split(',')[0], line.split(',')[2]) )\n",
    "questionRDDFiltered.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('Q1', 'What is ..?'), ('Q2', 'Who invented ..')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersRDD = sc.textFile(answers)\n",
    "answersRDD.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['A1,Q1,2015-01-02,It is ..',\n",
    " 'A2,Q2,2015-01-03,John Smith',\n",
    " 'A3,Q1,2015-01-05,I think it is ..']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to extract the second and the last column from the answersRDD\n",
    "answersRDDFiltered = answersRDD.map(lambda line : (line.split(',')[1], line.split(',')[3]) )\n",
    "answersRDDFiltered.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('Q1', 'It is ..'), ('Q2', 'John Smith'), ('Q1', 'I think it is ..')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRDDToMap = questionRDDFiltered.cogroup(answersRDDFiltered)\n",
    "finalRDD = finalRDDToMap.mapValues(lambda value : (list(value[0]), list(value[1]))  )\n",
    "finalRDD.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('Q1', (['What is ..?'], ['It is ..', 'I think it is ..'])),\n",
    " ('Q2', (['Who invented ..'], ['John Smith']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRDDToMap.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('Q1',\n",
    "  (<pyspark.resultiterable.ResultIterable at 0x7ff591434940>,\n",
    "   <pyspark.resultiterable.ResultIterable at 0x7ff591462580>)),\n",
    " ('Q2',\n",
    "  (<pyspark.resultiterable.ResultIterable at 0x7ff591418100>,\n",
    "   <pyspark.resultiterable.ResultIterable at 0x7ff591418d90>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalRDD.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting misleading users based on the watched movies and preferences expressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\"\n",
    "inputPath2  = \"./\"\n",
    "inputPath3  = \"./\"\n",
    "outputPath = \"./\" \n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchedMoviesRDD = sc.textFile(inputPath)\n",
    "watchedMoviesRDD.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['user1,movie1,201606061500,201606061650',\n",
    " 'user1,movie3,201606061800,201606061834',\n",
    " 'user1,movie4,201609061500,201609061605',\n",
    " 'user1,movie5,201610061100,201610061450',\n",
    " 'user2,movie6,201610081800,201610081845',\n",
    " 'user2,movie3,201610091800,201610091834',\n",
    " 'user2,movie4,201611051100,201611051105']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need only the pair (userId, Movie)\n",
    "usersAndMovies = watchedMoviesRDD.map(lambda line : (line.split(\",\")[1], line.split(\",\")[0]) )\n",
    "usersAndMovies.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('movie1', 'user1'),\n",
    " ('movie3', 'user1'),\n",
    " ('movie4', 'user1'),\n",
    " ('movie5', 'user1'),\n",
    " ('movie6', 'user2'),\n",
    " ('movie3', 'user2'),\n",
    " ('movie4', 'user2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moviesAndGenre = sc.textFile(inputPath2).map(lambda line:(line.split(\",\")[0], line.split(\",\")[2]))\n",
    "moviesAndGenre.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('movie1', 'Animation'),\n",
    " ('movie2', 'Adventure'),\n",
    " ('movie3', 'Comedy'),\n",
    " ('movie4', 'Comedy'),\n",
    " ('movie5', 'Comedy'),\n",
    " ('movie6', 'Action'),\n",
    " ('movie7', 'Comedy'),\n",
    " ('movie8', 'Adventure'),\n",
    " ('movie9', 'Action'),\n",
    " ('movie10', 'Action')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I need to join the 2 RDD\n",
    "usersAndGenres = usersAndMovies.join(moviesAndGenre).map(lambda line : line[1])\n",
    "usersAndGenres.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('user1', 'Comedy'),\n",
    " ('user1', 'Animation'),\n",
    " ('user1', 'Comedy'),\n",
    " ('user2', 'Comedy'),\n",
    " ('user1', 'Comedy'),\n",
    " ('user2', 'Comedy'),\n",
    " ('user2', 'Action')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = sc.textFile(inputPath3).map(lambda line : (line.split(\",\")[0], line.split(\",\")[1]))\n",
    "preferences.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('user1', 'Animation'), ('user1', 'Comedy'), ('user2', 'Action')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I want to understand if the user watched films of genres who did not specified\n",
    "# I can use the cogroup transforamtion to get for each users 2 lists\n",
    "# the first is the list of genres he watched (the true)\n",
    "# the second list is what ge declared (the possible fake)\n",
    "usersWithGenresAndPrefer = usersAndGenres.cogroup(preferences)\n",
    "usersWithGenresAndPrefer = usersWithGenresAndPrefer.mapValues(lambda value : (list(value[0]),list(value[1])))\n",
    "usersWithGenresAndPrefer.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[('user2', (['Comedy', 'Comedy', 'Action'], ['Action'])),\n",
    " ('user1',\n",
    "  (['Comedy', 'Animation', 'Comedy', 'Comedy'], ['Animation', 'Comedy']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def misleadingUser(line):\n",
    "    \n",
    "    # I need to compare this two lists and compute the percentage\n",
    "    # of genres watched not in the preference list\n",
    "    watchedGenre = line[1][0]\n",
    "    preferences = line[1][1]\n",
    "    \n",
    "    totalGenreWatched = 0\n",
    "    totalGenreLiked = 0\n",
    "    \n",
    "    for movie in watchedGenre:\n",
    "        totalGenreWatched += 1\n",
    "        for pref in preferences:\n",
    "            if(movie==pref):\n",
    "                totalGenreLiked += 1\n",
    "    \n",
    "    \n",
    "    if(((totalGenreWatched - totalGenreLiked)/totalGenreWatched) > threshold):\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misleadingUsers = usersWithGenresAndPrefer.filter(misleadingUser)\n",
    "misleadingUsers = misleadingUsers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misleadingUsers = misleadingUsers.coalesce(1)\n",
    "misleadingUsers.saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select window with an increasing trend (flatMap+groupByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "inputPath  = \"./\" \n",
    "outputPath = \"./\"\n",
    "inputRDD = sc.textFile(inputPath)\n",
    "inputRDD.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['1451606400,12.1',\n",
    " '1451606460,12.2',\n",
    " '1451606520,13.5',\n",
    " '1451606580,14.0',\n",
    " '1451606640,14.0',\n",
    " '1451606700,15.5',\n",
    " '1451606760,15.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to export the following key value pair from the input RDD\n",
    "# (timestamp of the record, [(timestamp, value), (timestamp-60, value), (timestamp-120, value)] )\n",
    "def extractWindow(reading):\n",
    "    timestamp = int(reading.split(\",\")[0])\n",
    "    temperature = float(reading.split(\",\")[1])\n",
    "    \n",
    "    window = []\n",
    "    \n",
    "    window.append((timestamp, reading))\n",
    "    window.append((timestamp-60, reading))\n",
    "    window.append((timestamp-120, reading))\n",
    "    \n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowTimestamp = inputRDD.flatMap(extractWindow)\n",
    "windowTimestamp.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[(1451606400, '1451606400,12.1'),\n",
    " (1451606340, '1451606400,12.1'),\n",
    " (1451606280, '1451606400,12.1'),\n",
    " (1451606460, '1451606460,12.2'),\n",
    " (1451606400, '1451606460,12.2'),\n",
    " (1451606340, '1451606460,12.2'),\n",
    " (1451606520, '1451606520,13.5'),\n",
    " (1451606460, '1451606520,13.5'),\n",
    " (1451606400, '1451606520,13.5'),\n",
    " (1451606580, '1451606580,14.0'),\n",
    " (1451606520, '1451606580,14.0'),\n",
    " (1451606460, '1451606580,14.0'),\n",
    " (1451606640, '1451606640,14.0'),\n",
    " (1451606580, '1451606640,14.0'),\n",
    " (1451606520, '1451606640,14.0'),\n",
    " (1451606700, '1451606700,15.5'),\n",
    " (1451606640, '1451606700,15.5'),\n",
    " (1451606580, '1451606700,15.5'),\n",
    " (1451606760, '1451606760,15.0'),\n",
    " (1451606700, '1451606760,15.0'),\n",
    " (1451606640, '1451606760,15.0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowTimestampPerKey = windowTimestamp.groupByKey()\n",
    "windowTimestampPerKey = windowTimestampPerKey.mapValues(lambda value : list(value))\n",
    "windowTimestampPerKey.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[(1451606400, ['1451606400,12.1', '1451606460,12.2', '1451606520,13.5']),\n",
    " (1451606340, ['1451606400,12.1', '1451606460,12.2']),\n",
    " (1451606280, ['1451606400,12.1']),\n",
    " (1451606460, ['1451606460,12.2', '1451606520,13.5', '1451606580,14.0']),\n",
    " (1451606520, ['1451606520,13.5', '1451606580,14.0', '1451606640,14.0']),\n",
    " (1451606580, ['1451606580,14.0', '1451606640,14.0', '1451606700,15.5']),\n",
    " (1451606640, ['1451606640,14.0', '1451606700,15.5', '1451606760,15.0']),\n",
    " (1451606700, ['1451606700,15.5', '1451606760,15.0']),\n",
    " (1451606760, ['1451606760,15.0'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used in the next transformation to select the windows with an incrasing temperature trend\n",
    "def increasingTrendFunc(pairInitialTimestampWindow):\n",
    "\n",
    "    # The key of the input pair is the intial timestamp of the current window\n",
    "    minTimestamp = pairInitialTimestampWindow[0]\n",
    "    \n",
    "    # Store the (at most) 3 elements of the window in a dictionary\n",
    "    # containing enties time stamp -> temperature\n",
    "    timestampTemp = {}\n",
    "\n",
    "    # pairInitialTimestampWindow[1] contains the elements of the current window\n",
    "    window = pairInitialTimestampWindow[1]\n",
    "    \n",
    "    \n",
    "    for timestampTemperature in window:\n",
    "        fields = timestampTemperature.split(\",\")\n",
    "        t = int(fields[0])\n",
    "        temperature = float(fields[1])\n",
    "        \n",
    "        timestampTemp[t] = temperature\n",
    "        \n",
    "    \n",
    "    # Check if the list contains three elements.\n",
    "    # If the number of elements is not equal to 3 the window is incomplete and must be discarded\n",
    "    if len(timestampTemp) != 3:\n",
    "        increasing = False\n",
    "    else:\n",
    "        # Check is the increasing trend is satisfied\n",
    "        if timestampTemp[minTimestamp]<timestampTemp[minTimestamp+60] and timestampTemp[minTimestamp+60]<timestampTemp[minTimestamp+120]:\n",
    "            increasing = True\n",
    "        else:\n",
    "            increasing = False\n",
    "            \n",
    "    return increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seletedWindowsRDD = windowTimestampPerKey.filter(increasingFunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is in the value part of the returned pairs\n",
    "seletedWindowsRDD.values().map(lambda window: list(window)).collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[['1451606400,12.1', '1451606460,12.2', '1451606520,13.5'],\n",
    " ['1451606460,12.2', '1451606520,13.5', '1451606580,14.0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the result. Map the iterable associated with each window to a list\n",
    "seletedWindowsRDD.values().map(lambda window: list(window)).saveAsTextFile(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe (selectExpr): discretized age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"./\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file profiles.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- surname: string (nullable = true)\n",
    "# |-- age: integer (nullable = true)\n",
    "profilesDF = spark.read.load(inputPath,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profilesDF.show()\n",
    "profilesDF.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- surname: string (nullable = true)\n",
    " |-- rangeage: string (nullable = true)\n",
    "\n",
    "+-----+-------+--------+\n",
    "| name|surname|rangeage|\n",
    "+-----+-------+--------+\n",
    "|Paolo|  Garza| [40-49]|\n",
    "| Luca| Boccia| [40-49]|\n",
    "|Maura|Bianchi| [10-19]|\n",
    "+-----+-------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User Defined Function called AgeCategory(age)\n",
    "# that returns a string associated with the Category of the user.\n",
    "# AgeCategory = \"[(age/10)*10-(age/10)*10+9]\"\n",
    "# e.g.,\n",
    "# 43 -> [40-49]\n",
    "# 39 -> [30-39]\n",
    "# 21 -> [20-29]\n",
    "# 17 -> [10-19]\n",
    "# ..\n",
    "spark.udf.register(\"AgeCategory\", lambda age: \"[\"+str((age//10)*10)+\"-\"+str((age//10)*10+9)+\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame with the following schema:\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- surname: string (nullable = true)\n",
    "# |-- rangeage: String (nullable = true)\n",
    "\n",
    "profilesDiscretizedAge = profilesDF.selectExpr(\"name\", \"surname\", \"AgeCategory(age) as rangeage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profilesDiscretizedAge.printSchema()\n",
    "profilesDiscretizedAge.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- surname: string (nullable = true)\n",
    " |-- rangeage: string (nullable = true)\n",
    "\n",
    "+-----+-------+--------+\n",
    "| name|surname|rangeage|\n",
    "+-----+-------+--------+\n",
    "|Paolo|  Garza| [40-49]|\n",
    "| Luca| Boccia| [40-49]|\n",
    "|Maura|Bianchi| [10-19]|\n",
    "+-----+-------+--------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "profilesDiscretizedAge.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe SQL: discretized age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"./\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file profiles.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- surname: string (nullable = true)\n",
    "# |-- age: integer (nullable = true)\n",
    "profilesDF = spark.read.load(inputPath,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User Defined Function called AgeCategory(age)\n",
    "# that returns a string associated with the Category of the user.\n",
    "# AgeCategory = \"[(age/10)*10-(age/10)*10+9]\"\n",
    "# e.g.,\n",
    "# 43 -> [40-49]\n",
    "# 39 -> [30-39]\n",
    "# 21 -> [20-29]\n",
    "# 17 -> [10-19]\n",
    "# ..\n",
    "spark.udf.register(\"AgeCategory\", lambda age: \"[\"+str((age//10)*10)+\"-\"+str((age//10)*10+9)+\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profilesDF.registerTempTable(\"profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame with the following schema:\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- surname: string (nullable = true)\n",
    "# |-- rangeage: String (nullable = true)\n",
    "profilesDiscretizedAge = spark.sql(\"\"\"SELECT name, surname, AgeCategory(age) as rangeage\n",
    "FROM profiles\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profilesDiscretizedAge.printSchema()\n",
    "#profilesDiscretizedAge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "profilesDiscretizedAge.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe (selectExpr): concatenation of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"./\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file profiles.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- surname: string (nullable = true)\n",
    "# |-- age: integer (nullable = true)\n",
    "profilesDF = spark.read.load(inputPath,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User Defined Function called Concatenate(name, surname)\n",
    "# that returns a string associated with the concatenation of name and surname.\n",
    "spark.udf.register(\"Concatenate\", lambda name, surname: name+\" \"+surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame with the following schema:\n",
    "# |-- name_surname: string (nullable = true)\n",
    "namesDF = profilesDF.selectExpr(\"Concatenate(name, surname) as name_surname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namesDF.printSchema()\n",
    "namesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "namesDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe SQL: concatenation of fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputPath = \"./\"\n",
    "outputPath = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of the input file profiles.csv and store it into a DataFrame\n",
    "# The input file has an header\n",
    "# Schema of the input data:\n",
    "# |-- name: string (nullable = true)\n",
    "# |-- surname: string (nullable = true)\n",
    "# |-- age: integer (nullable = true)\n",
    "profilesDF = spark.read.load(inputPath,\\\n",
    "                             format=\"csv\",\n",
    "                             header=True,\\\n",
    "                             inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a User Defined Function called Concatenate(name, surname)\n",
    "# that returns a string associated with the concatenation of name and surname.\n",
    "spark.udf.register(\"Concatenate\", lambda name, surname: name+\" \"+surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profilesDF.registerTempTable(\"profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DataFrame with the following schema:\n",
    "# |-- name_surname: string (nullable = true)\n",
    "namesDF = spark.sql(\"SELECT Concatenate(name, surname) as name_surname FROM profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#namesDF.printSchema()\n",
    "#namesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result in the output folder\n",
    "namesDF.write.csv(outputPath, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
