{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract stationId and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "linesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "fullReadingsDStream = linesDStream.filter(lambda line: int(line.split(\",\")[1])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stationid and timestamp\n",
    "stationIdTimestampDStream = fullReadingsDStream.map(lambda line: (line.split(\",\")[0],line.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationIdTimestampDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the selected readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "linesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "fullReadingsDStream = linesDStream.filter(lambda line: int(line.split(\",\")[1])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of selected readings\n",
    "numReadingsFullDStream = fullReadingsDStream.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numReadingsFullDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the stationId removing the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "linesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "fullReadingsDStream = linesDStream.filter(lambda line: int(line.split(\",\")[1])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the set of StationIDs\n",
    "stationIDsDuplicatesDStream = fullReadingsDStream.map(lambda line: line.split(\",\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "# distinct() is not avaiable for DStreams. We must use transfrom to remove duplicates\n",
    "distinctStationIDsDStream = stationIDsDuplicatesDStream.transform(lambda batchRDD: batchRDD.distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctStationIDsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the maximum number of free slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "linesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract # free slots\n",
    "freeSlotsDStream = linesDStream.map(lambda line: int(line.split(\",\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the maximum number of free slots\n",
    "maxFreeSlotsDStream = freeSlotsDStream.reduce(lambda v1, v2: max(v1,v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxFreeSlotsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 30 seconds print the stocks with a variation greater than 0.5% in the last 30 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "linesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer for each stockID the price variation (compute it for each batch).\n",
    "# Select only the stocks with a price variation (%) greater than 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return one pair (stockId, (price, price) )  for each input record\n",
    "\n",
    "def extractStockIdPricePrice(line):\n",
    "    fields = line.split(\",\")\n",
    "    \n",
    "    stockId = fields[1]\n",
    "    price = fields[2]\n",
    "    \n",
    "    return (stockId, (float(price), float(price)) )\n",
    "\n",
    "stockIdPriceDStream = linesDStream.map(extractStockIdPricePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max and min for each stockId\n",
    "stockIdMaxMinPriceDStream = stockIdPriceDStream\\\n",
    ".reduceByKey(lambda v1, v2: ( max(v1[0],v2[0]), min(v1[1],v2[1]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variation for each stock\n",
    "stockIdVariationDStream = stockIdMaxMinPriceDStream\\\n",
    ".mapValues(lambda MaxMinValue: 100.0*(MaxMinValue[0]-MaxMinValue[1])/MaxMinValue[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the stocks with variation greater than 0.5%\n",
    "selectedStockIdsVariationsDStream = stockIdVariationDStream.filter(lambda pair: pair[1]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedStockIdsVariationsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 30 seconds print the stocks with a variation greater than 0.5% in the last 60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "linesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer for each stockID the price variation (compute it for each batch).\n",
    "# Select only the stocks with a price variation (%) greater than 0.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return one pair (stockId, (price, price) )  for each input record\n",
    "\n",
    "def extractStockIdPricePrice(line):\n",
    "    fields = line.split(\",\")\n",
    "    \n",
    "    stockId = fields[1]\n",
    "    price = fields[2]\n",
    "    \n",
    "    return (stockId, (float(price), float(price)) )\n",
    "\n",
    "stockIdPriceDStream = linesDStream.map(extractStockIdPricePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max and min for each stockId\n",
    "# Set the windows zise to 60 seconds\n",
    "# The sliding interval is equal to 30 seconds, i.e., 1 batch\n",
    "stockIdMaxMinPriceDStream = stockIdPriceDStream\\\n",
    ".reduceByKeyAndWindow(lambda v1, v2: ( max(v1[0],v2[0]), min(v1[1],v2[1]) ), None, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variation for each stock\n",
    "stockIdVariationDStream = stockIdMaxMinPriceDStream\\\n",
    ".mapValues(lambda MaxMinValue: 100.0*(MaxMinValue[0]-MaxMinValue[1])/MaxMinValue[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the stocks with variation greater than 0.5%\n",
    "selectedStockIdsVariationsDStream = stockIdVariationDStream.filter(lambda pair: pair[1]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedStockIdsVariationsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 200 seconds\n",
    "ssc.awaitTerminationOrTimeout(200)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 2 seconds the name of the station in the last 2 seconds with 0 free slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileStations = \"stations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Standard\" RDD associated with the characteristics of the stations\n",
    "# Extract (stationId, name)\n",
    "stationNameRDD = sc.textFile(inputFileStations)\\\n",
    ".map(lambda line: (line.split(\"\\t\")[0], line.split(\"\\t\")[3]) ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "readingsDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each readings has the format:\n",
    "# stationId,#free slots,#used slots,timestamp\n",
    "# Select readings with num. free slots = 0\n",
    "fullReadingsDStream = readingsDStream.filter(lambda line: int(line.split(\",\")[1])==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pairs (stationId, timestamp)\n",
    "stationIdTimestampDStream = fullReadingsDStream.map(lambda line: (line.split(\",\")[0],line.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the content of the DStream with the \"standard\" RDD to retrieve\n",
    "# the name of each station. \n",
    "# To perform this join between streaming and\n",
    "# non-streaming RDDs the transform transformation must be used\n",
    "joinDStream = stationIdTimestampDStream.transform(lambda batchRDD: batchRDD.join(stationNameRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract (name of the station, timestamp)\n",
    "# It is the value part of the returned pairs\n",
    "stationNameTimestampDStream = joinDStream.map(lambda pair: pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationNameTimestampDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 60 seconds print the stock with a price lower than the historical minimum or higher than the historical maximum in the last 60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalInputFile = \"historicalData.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the historical data and compute the maximum and minimum price for each stock\n",
    "# Non-streaming RDD\n",
    "historicalDataRDD = sc.textFile(historicalInputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return one pair (stockId, (price, price) )  for each input record\n",
    "def extractStockIdPricePrice(line):\n",
    "    fields = line.split(\",\")\n",
    "    \n",
    "    stockId = fields[1]\n",
    "    price = fields[2]\n",
    "    \n",
    "    return (stockId, (float(price), float(price)) )\n",
    "\n",
    "stockIdPriceHistoricalRDD = historicalDataRDD.map(extractStockIdPricePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max and min for each stockId based on the historical data\n",
    "stockIdPriceHistoricalMaxMinRDD = stockIdPriceHistoricalRDD\\\n",
    ".reduceByKey(lambda v1, v2: ( max(v1[0],v2[0]), min(v1[1],v2[1]) ) ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "#ssc = StreamingContext(sc, 60)\n",
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "pricesDStream = ssc.socketTextStream(\"localhost\", 9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on the stockid each input record of the input stream with the \n",
    "# content of stockIdPriceHistoricalMaxMinRDD to retrieve \n",
    "# the historical maximum-minimum range of the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return one pair (stockId,price) for each input record\n",
    "stockIdPriceDStream = pricesDStream.map(lambda record: ( record.split(\",\")[1] , float(record.split(\",\")[2])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the RDD associated with the content of the current batch and \n",
    "# the non-streaming RDD stockIdPriceHistoricalMaxMinRDD\n",
    "stockIdPriceMaxMinDStream = stockIdPriceDStream\\\n",
    ".transform(lambda batchRDD: batchRDD.join(stockIdPriceHistoricalMaxMinRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only lines with price > maximum historical price \n",
    "# or price < minimum historical price\n",
    "def anomalyValue(pair):\n",
    "    currentPrice = pair[1][0]\n",
    "    stockHistoricalMaxPrice = pair[1][1][0]\n",
    "    stockHistoricalMinPrice = pair[1][1][1]\n",
    "    \n",
    "    if currentPrice>stockHistoricalMaxPrice or currentPrice<stockHistoricalMinPrice:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "selectedStockPricesDStream = stockIdPriceMaxMinDStream.filter(anomalyValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve only the stockIDs and apply distinct to remove duplicates\n",
    "# keys and distinct are not available for DStreams.\n",
    "# transform must be used\n",
    "selectStockIdsDStream = selectedStockPricesDStream\\\n",
    ".transform(lambda batchRDD: batchRDD.keys().distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectStockIdsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every 30 seconds print the stock with a price lower than the historical minimum or higher than the historical maximum in the last 60 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalInputFile = \"historicalData.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the historical data and compute the maximum and minimum price for each stock\n",
    "# Non-streaming RDD\n",
    "historicalDataRDD = sc.textFile(historicalInputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return one pair (stockId, (price, price) )  for each input record\n",
    "def extractStockIdPricePrice(line):\n",
    "    fields = line.split(\",\")\n",
    "    \n",
    "    stockId = fields[1]\n",
    "    price = fields[2]\n",
    "    \n",
    "    return (stockId, (float(price), float(price)) )\n",
    "\n",
    "stockIdPriceHistoricalRDD = historicalDataRDD.map(extractStockIdPricePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute max and min for each stockId based on the historical data\n",
    "stockIdPriceHistoricalMaxMinRDD = stockIdPriceHistoricalRDD\\\n",
    ".reduceByKey(lambda v1, v2: ( max(v1[0],v2[0]), min(v1[1],v2[1]) ) ).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark Streaming Context object\n",
    "#ssc = StreamingContext(sc, 30)\n",
    "ssc = StreamingContext(sc, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a (Receiver) DStream that will connect to localhost:9999\n",
    "# Specify that we want to apply a sliding window on the input stream\n",
    "# - windowDuration = 60 seconds\n",
    "# - slideDuration = 30 seconds\n",
    "pricesDStream = ssc.socketTextStream(\"localhost\", 9999)\\\n",
    ".window(10, 5)\n",
    "#.window(60, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on the stockid each input record of the input stream with the \n",
    "# content of stockIdPriceHistoricalMaxMinRDD to retrieve \n",
    "# the historical maximum-minimum range of the stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return one pair (stockId,price) for each input record\n",
    "stockIdPriceDStream = pricesDStream.map(lambda record: ( record.split(\",\")[1] , float(record.split(\",\")[2])) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the RDD associated with the content of the current batch and \n",
    "# the non-streaming RDD stockIdPriceHistoricalMaxMinRDD\n",
    "stockIdPriceMaxMinDStream = stockIdPriceDStream\\\n",
    ".transform(lambda batchRDD: batchRDD.join(stockIdPriceHistoricalMaxMinRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only lines with price > maximum historical price \n",
    "# or price < minimum historical price\n",
    "def anomalyValue(pair):\n",
    "    currentPrice = pair[1][0]\n",
    "    stockHistoricalMaxPrice = pair[1][1][0]\n",
    "    stockHistoricalMinPrice = pair[1][1][1]\n",
    "    \n",
    "    if currentPrice>stockHistoricalMaxPrice or currentPrice<stockHistoricalMinPrice:\n",
    "        return True\n",
    "    else:\n",
    "        return False   \n",
    "\n",
    "selectedStockPricesDStream = stockIdPriceMaxMinDStream.filter(anomalyValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve only the stockIDs and apply distinct to remove duplicates\n",
    "# keys and distinct are not available for DStreams.\n",
    "# transform must be used\n",
    "selectStockIdsDStream = selectedStockPricesDStream\\\n",
    ".transform(lambda batchRDD: batchRDD.keys().distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectStockIdsDStream.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the computation\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this application for 90 seconds\n",
    "ssc.awaitTerminationOrTimeout(90)\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
